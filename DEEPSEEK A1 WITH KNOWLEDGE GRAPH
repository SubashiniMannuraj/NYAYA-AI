import PyPDF2
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
import faiss
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from py2neo import Graph

def load_pdf_text(pdf_path):
    """Extract text from a PDF using PyPDF2."""
    text = ""
    with open(pdf_path, "rb") as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

pdf_path = "/content/ipc sec 420 final.pdf"
pdf_text = load_pdf_text(pdf_path)

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunks = text_splitter.split_text(pdf_text)

embedding_model = "sentence-transformers/all-mpnet-base-v2"
embeddings = HuggingFaceEmbeddings(model_name=embedding_model)

dimension = 768  # Embedding dimension for all-mpnet-base-v2
index = faiss.IndexFlatL2(dimension)  # L2 (Euclidean) distance index

batch_size = 100
for i in range(0, len(chunks), batch_size):
    batch = chunks[i : i + batch_size]
    batch_embeddings = embeddings.embed_documents(batch)
    index.add(np.array(batch_embeddings, dtype=np.float32))  # Add embeddings to FAISS index

faiss.write_index(index, "faiss_index.idx")  # Save the FAISS index

NEO4J_URI = "neo4j+s://6ecdc5f2.databases.neo4j.io"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "xXDQIQ3l3QzlFoQlUQpL4cAImYbQl_ALGoAx4wUZams"  # Replace with your actual password

graph = Graph(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))

for idx, chunk in enumerate(chunks):
    query = """
    MERGE (a:Article {id: $id, text: $text})
    """
    graph.run(query, id=idx, text=chunk)


def safe_merge_node(label, unique_key, properties):
    """
    Check if a node with the given label and unique property (unique_key) exists.
    If it does not, create the node with all given properties.
    """
    match_query = f"MATCH (n:{label} {{{unique_key}: $val}}) RETURN n"
    result = graph.run(match_query, val=properties[unique_key]).data()
    if result:
        return  # Node exists, do nothing.
    create_query = f"CREATE (n:{label} $props) RETURN n"
    graph.run(create_query, props=properties)

def create_extended_schema():
    """
    Create nodes for Person, Object, Event, Step, OnlinePortal, and ProcedureAfterApproach,
    and create relationships among them to represent the process.
    """
    graph.run("CREATE CONSTRAINT person_name_unique IF NOT EXISTS FOR (p:Person) REQUIRE p.name IS UNIQUE")
    graph.run("CREATE CONSTRAINT object_name_unique IF NOT EXISTS FOR (o:Object) REQUIRE o.name IS UNIQUE")
    graph.run("CREATE CONSTRAINT event_id_unique IF NOT EXISTS FOR (e:Event) REQUIRE e.id IS UNIQUE")
    graph.run("CREATE CONSTRAINT step_order_unique IF NOT EXISTS FOR (s:Step) REQUIRE s.order IS UNIQUE")
    graph.run("CREATE CONSTRAINT onlineportal_link_unique IF NOT EXISTS FOR (op:OnlinePortal) REQUIRE op.link IS UNIQUE")
    graph.run("CREATE CONSTRAINT procedure_id_unique IF NOT EXISTS FOR (ps:ProcedureAfterApproach) REQUIRE ps.id IS UNIQUE")


    safe_merge_node("Person", "name", {"name": "John Doe", "contact": "john.doe@example.com"})
    safe_merge_node("Object", "name", {"name": "Mobile Phone", "type": "Smartphone", "imei": "123456789012345"})
    safe_merge_node("Event", "id", {"id": "EVT001", "type": "Phone Theft", "date": "2023-08-01", "location": "Downtown"})


    steps = [
        {"order": 1, "description": "Lock the device remotely using Find My Device."},
        {"order": 2, "description": "Contact your mobile service provider to report the theft."},
        {"order": 3, "description": "File a police report with all relevant details."}
    ]
    for step in steps:
        safe_merge_node("Step", "order", step)


    safe_merge_node("OnlinePortal", "link", {
        "link": "https://www.policeonlineportal.example.com",
        "name": "Police Online Portal",
        "instructions": "Visit the portal, register your complaint by filling in your personal and incident details, attach any supporting documents, and submit the form."
    })


    safe_merge_node("ProcedureAfterApproach", "id", {
        "id": "PROC001",
        "description": "Submit complaint details and upload supporting documents."
    })



    graph.run("""
    MATCH (p:Person {name: $person}), (e:Event {id: $event_id})
    MERGE (p)-[:EXPERIENCED]->(e)
    """, person="John Doe", event_id="EVT001")


    graph.run("""
    MATCH (e:Event {id: $event_id}), (o:Object {name: $object})
    MERGE (e)-[:INVOLVES]->(o)
    """, event_id="EVT001", object="Mobile Phone")


    for step in steps:
        graph.run("""
        MATCH (e:Event {id: $event_id}), (s:Step {order: $order})
        MERGE (e)-[:REQUIRES_STEP]->(s)
        """, event_id="EVT001", order=step["order"])


    graph.run("""
    MATCH (e:Event {id: $event_id}), (op:OnlinePortal {link: $link})
    MERGE (e)-[:HAS_ONLINE_PORTAL]->(op)
    """, event_id="EVT001", link="https://www.policeonlineportal.example.com")


    graph.run("""
    MATCH (op:OnlinePortal {link: $link}), (proc:ProcedureAfterApproach {id: $proc_id})
    MERGE (op)-[:OFFERS_PROCEDURE]->(proc)
    """, link="https://www.policeonlineportal.example.com", proc_id="PROC001")


create_extended_schema()
print("✅ Extended Knowledge Graph Schema created with Person, Object, Event, Step, OnlinePortal, and ProcedureAfterApproach nodes.")

# --------------------------------------------------------------------------

model_name = "deepseek-ai/deepseek-llm-7b-chat"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    offload_folder="offload_weights"
)

print("✅ DeepSeek model loaded successfully!")

text_gen_pipeline = pipeline("text-generation", model=model, tokenizer=tokenizer)


import re

def retrieve_from_neo4j(question):
    """Retrieve relevant legal data from Neo4j and return distinct context."""
    query = """
    CALL db.index.fulltext.queryNodes('articlesIndex', $query) 
    YIELD node, score
    RETURN node.text AS context
    ORDER BY score DESC
    LIMIT 3
    """
    result = graph.run(query, query=question).data()

    if result:
        context_text = " ".join([row["context"] for row in result])

        # Extract only the relevant Q&A section
        match = re.search(r"(Q1: What should I do if my wallet or phone is stolen\?.*?Justification:.*?)$", 
                          context_text, re.DOTALL)

        if match:
            return match.group(1)  # Return only the relevant extracted portion

        return "Relevant legal answer not found."

    return "No relevant legal data found."


def query_rag(question):
    """Retrieve context from Neo4j and generate a structured answer."""
    context = retrieve_from_neo4j(question)

    prompt = f"""
    You are a legal AI assistant providing structured and clear legal advice. 
    Use the given context to answer the question concisely. Include official links where applicable.

    Context: {context}

    Question: {question}

    Answer:
    """

    response = text_gen_pipeline(prompt, max_new_tokens=300)  # Increase token limit
    return response[0]["generated_text"]

# Example Query
question = "My phone got stolen! What should I do?"
answer = query_rag(question)
print("Answer:", answer)
